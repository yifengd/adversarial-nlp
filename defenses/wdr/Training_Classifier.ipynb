{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKRSbmu4Hmag"
   },
   "source": [
    "# Training classifier\n",
    "\n",
    "This script can be used to train the classifier on original and adversarial samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XAxbz_6GqSkr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "from copy import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvP1fooCfR7D"
   },
   "source": [
    "# Loading and transforming data into logits differences\n",
    "\n",
    "The first step is transforming our dataframe into logits differences for each original and adversarial sentence. For this, it is required to execute the model for each sentence with substitutions as explained in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "m-5rqMHqfR7D",
    "outputId": "a6b89aff-9d14-4dff-e5e6-1959e315fc74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotten-tomatoes_pwws_distilbert.csv\n",
      "imdb_pwws_cnn.csv\n",
      "ag-news_pwws_distilbert.csv\n",
      "ag-news_pwws_cnn.csv\n",
      "ag-news_pwws_lstm.csv\n",
      "ag-news_kuleshov_distilbert.csv\n",
      "ag-news_pwws_bert.csv\n",
      "imdb_pwws_distilbert.csv\n",
      "imdb_alzantot_distilbert.csv\n",
      "rotten-tomatoes_alzantot_distilbert.csv\n",
      "yelp-polarity_textfooler_bert.csv\n",
      "imdb_kuleshov_distilbert.csv\n",
      "ag-news_alzantot_distilbert.csv\n",
      "imdb_pwws_bert.csv\n",
      "imdb_pwws_lstm.csv\n",
      "yelp-polarity_pwws_bert.csv\n",
      "ag-news_textfooler_distilbert.csv\n",
      "imdb_textfooler_distilbert.csv\n"
     ]
    }
   ],
   "source": [
    "# Print available setups for testing\n",
    "for i in os.listdir('../../Generating Adversarial Samples/Data'):\n",
    "    if not i.startswith('.'): # Don't print system files\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "x7lVq4pAfR7E"
   },
   "outputs": [],
   "source": [
    "# Select the configuration for training\n",
    "test_config = 'imdb_pwws_distilbert.csv' # or 'agnews_pwws_distilbert.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "n3Iqx5dFfR7E",
    "outputId": "99c6827a-31ac-4d48-91bd-95bfbbb2d2f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture: distilbert\n",
      "Dataset: imdb\n"
     ]
    }
   ],
   "source": [
    "# Obtain model from test config\n",
    "model_arch = test_config.replace(\".csv\", \"\").split('_')[-1]\n",
    "dataset = test_config.split('_')[0]\n",
    "print(\"Model architecture:\", model_arch)\n",
    "print(\"Dataset:\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t4Q51W0ofR7F"
   },
   "outputs": [],
   "source": [
    "def load_textattack_local_model(model_arch, dataset):\n",
    "    \n",
    "    def load_module_from_file(file_path):\n",
    "        \"\"\"Uses ``importlib`` to dynamically open a file and load an object from\n",
    "        it.\"\"\"\n",
    "        temp_module_name = f\"temp_{time.time()}\"\n",
    "\n",
    "        spec = importlib.util.spec_from_file_location(temp_module_name, file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module\n",
    "    \n",
    "    m = load_module_from_file(f'../{model_arch}_{dataset}_textattack.py')\n",
    "    model = getattr(m, 'model')\n",
    "    \n",
    "    return model, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "90TzS-60fR7F"
   },
   "outputs": [],
   "source": [
    "def load_hugging_face_model(model_arch, dataset):\n",
    "    # Import the model used for generating the adversarial samples.\n",
    "    # Correctly, set up imports, model and tokenizer depending on the model you generated the samples on.\n",
    "    \n",
    "    if model_arch == 'distilbert':\n",
    "        from transformers import DistilBertConfig as config, DistilBertTokenizer as tokenizer, AutoModelForSequenceClassification as auto_model\n",
    "    elif model_arch == 'bert':\n",
    "        from transformers import BertConfig as config, BertTokenizer as tokenizer, AutoModelForSequenceClassification as auto_model\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tokenizer = tokenizer.from_pretrained(f\"textattack/{model_arch}-base-uncased-{dataset}\")\n",
    "    model = auto_model.from_pretrained(f\"textattack/{model_arch}-base-uncased-{dataset}\").to(device)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iujKfMatfR7G"
   },
   "outputs": [],
   "source": [
    "# Models available in hugging-face are executed differently from LSTM and CNN. Choose automatically the configuration and load model + tokenizer.\n",
    "textattack_local_models = ['lstm', 'cnn']\n",
    "\n",
    "if model_arch in textattack_local_models:\n",
    "    hugging_face_model = False\n",
    "    model, tokenizer = load_textattack_local_model(model_arch, dataset)\n",
    "\n",
    "else:\n",
    "    hugging_face_model = True\n",
    "    model, tokenizer = load_hugging_face_model(model_arch, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1XxOsu_4EgE"
   },
   "source": [
    "# Loading data\n",
    "\n",
    "Read into a dataframe your original and adversarial samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "twkc6DQIfR7G",
    "outputId": "7e163046-bf5c-4ec3-b5fd-06e577f11ab3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28492, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the desired csv file previously generated\n",
    "df = pd.read_csv(f'../../Generating Adversarial Samples/Data/{test_config}', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h3snuVjzfR7H"
   },
   "outputs": [],
   "source": [
    "# Select first entries. Only 3000 will be used but we leave room for false adversarial sentences that will be filtered out later and test set. We reduce size because computations are expensive.\n",
    "# In real setup, the whole file was considered and fixed train and test sets were produced.\n",
    "df = df.head(7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "y87VzvT1fR7H"
   },
   "outputs": [],
   "source": [
    "# Create batches of non-adversarial sentences\n",
    "# For big models such as BERT, we must divide our input in smaller batches.\n",
    "n = 256 # Size of each batch.\n",
    "batches = [list(df.original_text.values)[i:i + n] for i in range(0, len(df.original_text.values), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I recently purchased the complete American Gothic series on DVD and it lived up to my memories of it. I was very grateful to be able to view for the first time episodes that were never televised. I loved \"Ring of Fire\" in particular of the stories I hadn\\'t seen the first time around.  Gary Cole is fantastic as \"evil, sexy\" Lucas Buck. Lucas Black as Caleb is also a superb player. I thought Brenda Bakke as Selena Coombs was also superb in her portrayal. In fact, the whole cast was fantastically talented and had great chemistry with each other.  It\\'s a shame the series was screwed by the network (in collusion with a burgeoning group of censors) because it was truly designed for adult viewing. A mixture of comedy, tragedy, farce, satire, Gothic romance and horror genres, it offered brilliant characterizations supported by acting at the genius level.  I had the most tremendous lust for the devil for once in my life. Long live Gary Cole (Sheriff Lucas Buck), the most luscious \"fallen angel\" ever.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9w_oE8-fR7H"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all non-adversarial sentences in our dataset\n",
    "outputs = []\n",
    "\n",
    "if hugging_face_model is True: # Use tokenizer and hugging face pipeline\n",
    "    for b in batches: \n",
    "        input = tokenizer(b, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**input)\n",
    "            outputs.append(output.logits.cpu().numpy())\n",
    "            del input\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "else: # Use local model by simply predicting without tokenization\n",
    "    for b in batches: \n",
    "        output = model(b)\n",
    "        outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frRXf0P9fR7I"
   },
   "outputs": [],
   "source": [
    "# Obtain non-adversarial predictions\n",
    "outputs_flatten = [item for sublist in outputs for item in sublist]\n",
    "predictions = [np.argmax(i) for i in outputs_flatten]\n",
    "\n",
    "# Include prediction for these classes in our DataFrame\n",
    "df['original_class_predicted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqDSHW_nfR7I"
   },
   "outputs": [],
   "source": [
    "# Repeat process for adversarial sentences\n",
    "n = 256\n",
    "batches = [list(df.adversarial_text.values)[i:i + n] for i in range(0, len(df.adversarial_text.values), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrnkZkTdfR7J"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all non-adversarial sentences in our dataset\n",
    "outputs = []\n",
    "\n",
    "if hugging_face_model is True: # Use tokenizer and hugging face pipeline\n",
    "    for b in batches: \n",
    "        input = tokenizer(b, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**input)\n",
    "            outputs.append(output.logits.cpu().numpy())\n",
    "            del input\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "else: # Use local model by simply predicting without tokenization\n",
    "    for b in batches: \n",
    "        output = model(b)\n",
    "        outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1vAsWe9fR7J"
   },
   "outputs": [],
   "source": [
    "# Obtain adversarial predictions\n",
    "outputs_flatten = [item for sublist in outputs for item in sublist]\n",
    "predictions = [np.argmax(i) for i in outputs_flatten]\n",
    "\n",
    "# Include prediction for these classes in our DataFrame\n",
    "df['adversarial_class_predicted'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCUrHP-4fR7J"
   },
   "outputs": [],
   "source": [
    "# Select only those sentences for which there was actually a change in the prediction\n",
    "correct = df[(df['original_class_predicted'] != df['adversarial_class_predicted'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kzz81OGvfR7J"
   },
   "outputs": [],
   "source": [
    "# Update dataframe and keep only adversarial samples\n",
    "df = correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INb_NMpLfR7K"
   },
   "source": [
    "# Obtain logits\n",
    "Once we have the predictions and actually adversarial sentences, we generate the logits differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FiJMYa7fR7L"
   },
   "outputs": [],
   "source": [
    "original_samples = df.original_text.values\n",
    "adversarial_samples = df.adversarial_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PF7zyw_MfR7L"
   },
   "outputs": [],
   "source": [
    "# Concatenate all original samples and their predictions\n",
    "x = np.concatenate((original_samples, adversarial_samples))\n",
    "y = np.concatenate((np.zeros(len(original_samples)), np.ones(len(adversarial_samples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVzO4b6MfR7L"
   },
   "outputs": [],
   "source": [
    "def obtain_logits(samples, batch_size, model, tokenizer):\n",
    "    \"\"\"\n",
    "    For given samples and model, compute prediction logits.\n",
    "    Input data is splitted in batches.\n",
    "    \"\"\"\n",
    "    batches = [samples[i:i + batch_size] for i in range(0, len(samples), batch_size)]\n",
    "    logits = []\n",
    "\n",
    "    for i, b in enumerate(batches):\n",
    "        print(\"{}/{}\".format(i+1, len(batches)))\n",
    "        if hugging_face_model:\n",
    "            with torch.no_grad():\n",
    "                input = tokenizer(list(b), return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "                logits.append(model(**input).logits.cpu().numpy())\n",
    "        else:\n",
    "            logits.append(model(b))\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQ229UFNfR7L",
    "outputId": "84645530-d03c-4c20-9f42-083edf2c3118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n"
     ]
    }
   ],
   "source": [
    "# Compute logits for original sentences\n",
    "batch_size = 350\n",
    "original_logits = obtain_logits(original_samples, batch_size, model, tokenizer)\n",
    "original_logits = np.concatenate(original_logits).reshape(-1, original_logits[0].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTj7PW_2fR7M"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xG4vLDpifR7M",
    "outputId": "34ee5014-3d58-4e83-de23-92034add95c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n"
     ]
    }
   ],
   "source": [
    "# Compute logits for adversarial sentences\n",
    "batch_size = 350\n",
    "adversarial_logits = obtain_logits(adversarial_samples, batch_size, model, tokenizer)\n",
    "adversarial_logits = np.concatenate(adversarial_logits).reshape(-1, adversarial_logits[0].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c638-0iafR7M"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywMSEETffR7M"
   },
   "outputs": [],
   "source": [
    "# Concatenate all logits\n",
    "logits = np.concatenate((original_logits, adversarial_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnSx8bUufR7M"
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "import random\n",
    "c = list(zip(x, y, logits))\n",
    "random.shuffle(c)\n",
    "x, y, logits = zip(*c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4TaXtSZfR7N"
   },
   "source": [
    "## Computing logits difference\n",
    "\n",
    "This is a key step implemented. The main idea is:\n",
    "* For each sentence, replace each word by the `[UNK]` token and compute prediction logits\n",
    "* Using these logits, we can easily compute the saliency of the word as presented in the report.\n",
    "* Then, we sort words by descending saliency.\n",
    "* Finally, compute logits difference for each replacement. This difference is computed as `Logit from class predicted for the whole sentence - Highest remaining logit`\n",
    "\n",
    "More details on these derivations are found in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8ScyI0afR7N"
   },
   "outputs": [],
   "source": [
    "def compute_logits_difference(x, logits, y, model, tokenizer, idx, max_sentence_size=512):\n",
    "    n_classes = len(logits[idx])\n",
    "    predicted_class = np.argmax(logits[idx]) # Predicted class for whole sentence using previously computed logits\n",
    "    class_logit = logits[idx][predicted_class] # Store this origianl prediction logit\n",
    "\n",
    "    split_sentence = x[idx].split(' ')[:max_sentence_size] # The tokenizer will only consider 512 words so we avoid computing innecessary logits\n",
    "\n",
    "    new_sentences = []\n",
    "\n",
    "    # Here, we replace each word by [UNK] and generate all sentences to consider\n",
    "    for i, word in enumerate(split_sentence):\n",
    "        new_sentence = copy(split_sentence)\n",
    "        new_sentence[i] = '[UNK]'\n",
    "        new_sentence = ' '.join(new_sentence)\n",
    "        new_sentences.append(new_sentence)\n",
    "\n",
    "    # We cannot run more than 350 predictions simultaneously because of resources.\n",
    "    # Split in batches if necessary.\n",
    "    # Compute logits for all replacements.\n",
    "    if len(new_sentences) > 200:\n",
    "        logits = []\n",
    "        batches = [new_sentences[i:i + 200] for i in range(0, len(new_sentences), 200)]\n",
    "        for b in batches:\n",
    "            if hugging_face_model: # Use hugging face predictions\n",
    "                batch = tokenizer(b, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "                with torch.no_grad():\n",
    "                    logits.append(model(**batch).logits)\n",
    "            else:\n",
    "                logits.append(model(b).to(device))\n",
    "      \n",
    "        if hugging_face_model:\n",
    "            logits = torch.cat(logits)\n",
    "        else:\n",
    "            logits = np.concatenate( logits, axis=0 )\n",
    "            logits = torch.Tensor(logits)\n",
    "    \n",
    "    else: # There's no need to split in batches\n",
    "        if hugging_face_model:\n",
    "            batch = tokenizer(new_sentences, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**batch).logits\n",
    "            del batch\n",
    "        else:\n",
    "            logits = model(new_sentences)\n",
    "            logits = torch.Tensor(logits)\n",
    "\n",
    "\n",
    "    # Compute saliency\n",
    "    saliency = (class_logit - logits[:,predicted_class]).reshape(-1, 1)\n",
    "\n",
    "    # Append to logits for sorting\n",
    "    data = torch.cat((logits, saliency), 1)\n",
    "\n",
    "    # Sort by descending saliency\n",
    "    data = torch.stack(sorted(data, key=lambda a: a[n_classes], reverse=True))\n",
    "\n",
    "    # Remove saliency\n",
    "    data = data[:, :n_classes]\n",
    "\n",
    "    # Fix order: originallly predicted class, other classes\n",
    "    order = [predicted_class] + [i for i in range(n_classes) if i!=predicted_class]\n",
    "    data = torch.index_select(data, 1, torch.LongTensor(order).to(device))\n",
    "\n",
    "    # Compute difference between predicted class (always first column) and higher remaining logit\n",
    "    data = data[:, :1].flatten() - torch.max(data[:, 1:], dim=1).values.flatten()\n",
    "\n",
    "    del saliency\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Return only logits difference\n",
    "    return data.reshape(-1, 1), torch.Tensor([y[idx]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dQxs9ETfR7N"
   },
   "outputs": [],
   "source": [
    "def compute_logits_difference_padding(x, logits, y, model, tokenizer, idx, target_size=512):\n",
    "    \"\"\"\n",
    "    This function provides a wrapper for compute_logits_difference and includes padding to computations.\n",
    "    \"\"\"\n",
    "    data, y = compute_logits_difference(x, logits, y, model, tokenizer, idx, target_size)\n",
    "    data_size = min(512, data.shape[0])\n",
    "    target = torch.zeros(target_size, 1).to(device)\n",
    "    target[:data_size, :] = data\n",
    "\n",
    "    return target, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBrFnu75fR7O"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Text(Dataset):\n",
    "    \"\"\"\n",
    "    Dataloader following torch details. Each time we get an item, we will compute\n",
    "    the logits difference.\n",
    "    \"\"\"\n",
    "    def __init__(self, x , logits, y, model, tokenizer, train=True, max_sentence_size=512):\n",
    "        self.logits = logits\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sentence_size = max_sentence_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, y = compute_logits_difference_padding(self.x, self.logits, self.y, self.model, self.tokenizer, idx, self.max_sentence_size)\n",
    "        data = data[:, :1].unsqueeze(0)\n",
    "\n",
    "        return data, y, self.x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbwMfB4XfR7O"
   },
   "outputs": [],
   "source": [
    "# Create the dataloader\n",
    "train_ds = Text(x, logits, y, model, tokenizer)\n",
    "train_loader = DataLoader(dataset=train_ds, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7YUtzPZfR7O"
   },
   "outputs": [],
   "source": [
    "# Define the target DataFrame to structure our data.\n",
    "# It has a column for each input dimension (up to 512) and \n",
    "# it also includes whether it is adversarial or not (y_label) and the sentence from which the logits where extracted\n",
    "\n",
    "data_train = pd.DataFrame(columns=[i for i in range(512)]+['y_label', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTtHK3nnfR7P",
    "outputId": "d48412ae-2f8a-43cb-ecaa-31d2144360b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1 - 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate logits difference by running the loader.\n",
    "for i, (data, y_label, sentence) in enumerate(train_loader):\n",
    "    print(\"{}/{} - {}\\n\".format(i, len(train_loader), i/len(train_loader)))\n",
    "    for v in range(len(data)):\n",
    "        # Structure data and include in dataframe\n",
    "        row = np.append(data[v].cpu().numpy().reshape(1,-1), np.array([y_label[v].item(), sentence[v]]))\n",
    "        data_train = data_test.append(pd.DataFrame([row], columns=list(data_test)), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfCAOvbHfR7Z"
   },
   "outputs": [],
   "source": [
    "# Divide train and test set\n",
    "data_train = data_train.head(3000)\n",
    "data_test = data_train.tail(len(data_train)-3001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3WCIbfDGoqG"
   },
   "source": [
    "# Model training and comparison\n",
    "\n",
    "We train different models and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ftPvIuxmBlS"
   },
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJE-47xY2u_F",
    "outputId": "8f111d92-c95a-4b11-da17-3510a9769866"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=2, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1600,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model using best parameters found\n",
    "model = RandomForestClassifier(n_estimators=1600,\n",
    "                               min_samples_split=10,\n",
    "                               min_samples_leaf=2,\n",
    "                               max_features='auto',\n",
    "                               max_depth=None, \n",
    "                               bootstrap = True)\n",
    "# Fit on training data\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Zgrj0u_3PNM"
   },
   "outputs": [],
   "source": [
    "# Actual class predictions\n",
    "rf_predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0163kTVX3Sjo",
    "outputId": "a559459b-6a53-452e-8a42-33182cefeb3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.914"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(rf_predictions==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuD9VPNdAxh5",
    "outputId": "33935fea-a99e-4589-c721-c31a8d6c8797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.932     0.891     0.911       248\n",
      "         1.0      0.897     0.937     0.917       252\n",
      "\n",
      "    accuracy                          0.914       500\n",
      "   macro avg      0.915     0.914     0.914       500\n",
      "weighted avg      0.915     0.914     0.914       500\n",
      "\n",
      "[[221  27]\n",
      " [ 16 236]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, rf_predictions, digits=3))\n",
    "print(confusion_matrix(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji70i7tT4Z4B"
   },
   "source": [
    "### XGBoost\n",
    "\n",
    "Best performing model. Hyperparamter tuning done with Dataiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDNzQp9d4PP0"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyIrUTHJ4i8p"
   },
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(\n",
    "                    max_depth=3,\n",
    "                    learning_rate=0.34281802,\n",
    "                    gamma=0.6770816,\n",
    "                    min_child_weight=2.5520658,\n",
    "                    max_delta_step=0.71469694,\n",
    "                    subsample=0.61460966,\n",
    "                    colsample_bytree=0.73929816,\n",
    "                    colsample_bylevel=0.87191725,\n",
    "                    reg_alpha=0.9064181,\n",
    "                    reg_lambda=0.5686102,\n",
    "                    n_estimators=29,\n",
    "                    silent=0,\n",
    "                    nthread=4,\n",
    "                    scale_pos_weight=1.0,\n",
    "                    base_score=0.5,\n",
    "                    missing=None,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFRBurXZ4ohr",
    "outputId": "ec44d718-3d95-42b3-e2e4-37a225fc1db5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.87191725,\n",
       "              colsample_bynode=1, colsample_bytree=0.73929816, gamma=0.6770816,\n",
       "              learning_rate=0.34281802, max_delta_step=0.71469694, max_depth=3,\n",
       "              min_child_weight=2.5520658, missing=None, n_estimators=29,\n",
       "              n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0.9064181, reg_lambda=0.5686102, scale_pos_weight=1.0,\n",
       "              seed=1337, silent=0, subsample=0.61460966, verbosity=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfqAluu-4qik"
   },
   "outputs": [],
   "source": [
    "xgb_predictions = xgb_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wla48ARQA8Ff",
    "outputId": "0675d583-26cf-4d3c-d84b-d44ba3ea6ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.949     0.895     0.921       248\n",
      "         1.0      0.902     0.952     0.927       252\n",
      "\n",
      "    accuracy                          0.924       500\n",
      "   macro avg      0.925     0.924     0.924       500\n",
      "weighted avg      0.925     0.924     0.924       500\n",
      "\n",
      "[[222  26]\n",
      " [ 12 240]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, xgb_predictions, digits=3))\n",
    "print(confusion_matrix(y_test, xgb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7WrC0LE493U"
   },
   "source": [
    "### AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "706A1Vqd43Qf"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-Yl5c-A5BSu"
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoE3Ku7x5Fic",
    "outputId": "be4ef664-a99d-429f-fbc7-70442eaa8362"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxBWTfvA5Snt"
   },
   "outputs": [],
   "source": [
    "abc_predictions = abc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvMmHP8X5WD9",
    "outputId": "766a5870-b977-48e6-8872-9cbb709b9afe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(abc_predictions==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbDTgEtUBfWB",
    "outputId": "81011d9b-b3fd-44b8-83ba-8358eac91e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.956     0.875     0.914       248\n",
      "         1.0      0.886     0.960     0.922       252\n",
      "\n",
      "    accuracy                          0.918       500\n",
      "   macro avg      0.921     0.918     0.918       500\n",
      "weighted avg      0.921     0.918     0.918       500\n",
      "\n",
      "[[217  31]\n",
      " [ 10 242]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, abc_predictions, digits=3))\n",
    "print(confusion_matrix(y_test, abc_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnN-pUktttZ_"
   },
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTtL5YVVtvEV"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlfaoFqptxLJ"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'objective': 'binary',\n",
    "    'application': 'binary',\n",
    "    'metric': ['binary_logloss'],\n",
    "    'num_leaves': 35,\n",
    "    'learning_rate': 0.13,\n",
    "    'verbose': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClFNYu-OuVWM"
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(x, label=y)\n",
    "test_data = lgb.Dataset(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqhNtf00uKwP",
    "outputId": "7d9ab187-1558-4e07-e2c5-8d510d90b13b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.597939\n",
      "[2]\tvalid_0's binary_logloss: 0.525612\n",
      "[3]\tvalid_0's binary_logloss: 0.469367\n",
      "[4]\tvalid_0's binary_logloss: 0.423547\n",
      "[5]\tvalid_0's binary_logloss: 0.387778\n",
      "[6]\tvalid_0's binary_logloss: 0.357721\n",
      "[7]\tvalid_0's binary_logloss: 0.331829\n",
      "[8]\tvalid_0's binary_logloss: 0.310683\n",
      "[9]\tvalid_0's binary_logloss: 0.293977\n",
      "[10]\tvalid_0's binary_logloss: 0.280445\n",
      "[11]\tvalid_0's binary_logloss: 0.268066\n",
      "[12]\tvalid_0's binary_logloss: 0.257237\n",
      "[13]\tvalid_0's binary_logloss: 0.248693\n",
      "[14]\tvalid_0's binary_logloss: 0.241157\n",
      "[15]\tvalid_0's binary_logloss: 0.23551\n",
      "[16]\tvalid_0's binary_logloss: 0.230435\n",
      "[17]\tvalid_0's binary_logloss: 0.22547\n",
      "[18]\tvalid_0's binary_logloss: 0.223468\n",
      "[19]\tvalid_0's binary_logloss: 0.22089\n",
      "[20]\tvalid_0's binary_logloss: 0.219927\n",
      "[21]\tvalid_0's binary_logloss: 0.218465\n",
      "[22]\tvalid_0's binary_logloss: 0.21647\n",
      "[23]\tvalid_0's binary_logloss: 0.214435\n",
      "[24]\tvalid_0's binary_logloss: 0.213436\n",
      "[25]\tvalid_0's binary_logloss: 0.21353\n",
      "[26]\tvalid_0's binary_logloss: 0.214022\n",
      "[27]\tvalid_0's binary_logloss: 0.21455\n",
      "[28]\tvalid_0's binary_logloss: 0.214332\n",
      "[29]\tvalid_0's binary_logloss: 0.213994\n",
      "[30]\tvalid_0's binary_logloss: 0.214365\n",
      "[31]\tvalid_0's binary_logloss: 0.215326\n",
      "[32]\tvalid_0's binary_logloss: 0.2159\n",
      "[33]\tvalid_0's binary_logloss: 0.217941\n",
      "[34]\tvalid_0's binary_logloss: 0.220012\n",
      "[35]\tvalid_0's binary_logloss: 0.221169\n",
      "[36]\tvalid_0's binary_logloss: 0.221901\n",
      "[37]\tvalid_0's binary_logloss: 0.2228\n",
      "[38]\tvalid_0's binary_logloss: 0.224233\n",
      "[39]\tvalid_0's binary_logloss: 0.225725\n",
      "[40]\tvalid_0's binary_logloss: 0.227135\n",
      "[41]\tvalid_0's binary_logloss: 0.227536\n",
      "[42]\tvalid_0's binary_logloss: 0.227412\n",
      "[43]\tvalid_0's binary_logloss: 0.228044\n",
      "[44]\tvalid_0's binary_logloss: 0.228545\n",
      "[45]\tvalid_0's binary_logloss: 0.228542\n",
      "[46]\tvalid_0's binary_logloss: 0.229563\n",
      "[47]\tvalid_0's binary_logloss: 0.230307\n",
      "[48]\tvalid_0's binary_logloss: 0.230962\n",
      "[49]\tvalid_0's binary_logloss: 0.23147\n",
      "[50]\tvalid_0's binary_logloss: 0.232445\n",
      "[51]\tvalid_0's binary_logloss: 0.233729\n",
      "[52]\tvalid_0's binary_logloss: 0.234104\n",
      "[53]\tvalid_0's binary_logloss: 0.235964\n",
      "[54]\tvalid_0's binary_logloss: 0.237279\n",
      "[55]\tvalid_0's binary_logloss: 0.237751\n",
      "[56]\tvalid_0's binary_logloss: 0.237972\n",
      "[57]\tvalid_0's binary_logloss: 0.238326\n",
      "[58]\tvalid_0's binary_logloss: 0.239714\n",
      "[59]\tvalid_0's binary_logloss: 0.24034\n",
      "[60]\tvalid_0's binary_logloss: 0.240946\n",
      "[61]\tvalid_0's binary_logloss: 0.243098\n",
      "[62]\tvalid_0's binary_logloss: 0.24568\n",
      "[63]\tvalid_0's binary_logloss: 0.245631\n",
      "[64]\tvalid_0's binary_logloss: 0.246555\n",
      "[65]\tvalid_0's binary_logloss: 0.24661\n",
      "[66]\tvalid_0's binary_logloss: 0.248235\n",
      "[67]\tvalid_0's binary_logloss: 0.249583\n",
      "[68]\tvalid_0's binary_logloss: 0.250729\n",
      "[69]\tvalid_0's binary_logloss: 0.25111\n",
      "[70]\tvalid_0's binary_logloss: 0.252287\n",
      "[71]\tvalid_0's binary_logloss: 0.252956\n",
      "[72]\tvalid_0's binary_logloss: 0.253684\n",
      "[73]\tvalid_0's binary_logloss: 0.254974\n",
      "[74]\tvalid_0's binary_logloss: 0.257055\n",
      "[75]\tvalid_0's binary_logloss: 0.257503\n",
      "[76]\tvalid_0's binary_logloss: 0.258727\n",
      "[77]\tvalid_0's binary_logloss: 0.258623\n",
      "[78]\tvalid_0's binary_logloss: 0.260144\n",
      "[79]\tvalid_0's binary_logloss: 0.261282\n",
      "[80]\tvalid_0's binary_logloss: 0.262286\n",
      "[81]\tvalid_0's binary_logloss: 0.263384\n",
      "[82]\tvalid_0's binary_logloss: 0.263874\n",
      "[83]\tvalid_0's binary_logloss: 0.264223\n",
      "[84]\tvalid_0's binary_logloss: 0.265304\n",
      "[85]\tvalid_0's binary_logloss: 0.265796\n",
      "[86]\tvalid_0's binary_logloss: 0.267569\n",
      "[87]\tvalid_0's binary_logloss: 0.268912\n",
      "[88]\tvalid_0's binary_logloss: 0.269089\n",
      "[89]\tvalid_0's binary_logloss: 0.270984\n",
      "[90]\tvalid_0's binary_logloss: 0.271197\n",
      "[91]\tvalid_0's binary_logloss: 0.273018\n",
      "[92]\tvalid_0's binary_logloss: 0.274225\n",
      "[93]\tvalid_0's binary_logloss: 0.275077\n",
      "[94]\tvalid_0's binary_logloss: 0.27673\n",
      "[95]\tvalid_0's binary_logloss: 0.277752\n",
      "[96]\tvalid_0's binary_logloss: 0.278937\n",
      "[97]\tvalid_0's binary_logloss: 0.279931\n",
      "[98]\tvalid_0's binary_logloss: 0.281698\n",
      "[99]\tvalid_0's binary_logloss: 0.283304\n",
      "[100]\tvalid_0's binary_logloss: 0.286116\n",
      "[101]\tvalid_0's binary_logloss: 0.285848\n",
      "[102]\tvalid_0's binary_logloss: 0.285932\n",
      "[103]\tvalid_0's binary_logloss: 0.287541\n",
      "[104]\tvalid_0's binary_logloss: 0.289558\n",
      "[105]\tvalid_0's binary_logloss: 0.289793\n",
      "[106]\tvalid_0's binary_logloss: 0.291602\n",
      "[107]\tvalid_0's binary_logloss: 0.291488\n",
      "[108]\tvalid_0's binary_logloss: 0.292978\n",
      "[109]\tvalid_0's binary_logloss: 0.29434\n",
      "[110]\tvalid_0's binary_logloss: 0.296043\n",
      "[111]\tvalid_0's binary_logloss: 0.297743\n",
      "[112]\tvalid_0's binary_logloss: 0.298618\n",
      "[113]\tvalid_0's binary_logloss: 0.300189\n",
      "[114]\tvalid_0's binary_logloss: 0.302157\n",
      "[115]\tvalid_0's binary_logloss: 0.302707\n",
      "[116]\tvalid_0's binary_logloss: 0.304394\n",
      "[117]\tvalid_0's binary_logloss: 0.305014\n",
      "[118]\tvalid_0's binary_logloss: 0.306667\n",
      "[119]\tvalid_0's binary_logloss: 0.307361\n",
      "[120]\tvalid_0's binary_logloss: 0.309253\n",
      "[121]\tvalid_0's binary_logloss: 0.310109\n",
      "[122]\tvalid_0's binary_logloss: 0.311209\n",
      "[123]\tvalid_0's binary_logloss: 0.312531\n",
      "[124]\tvalid_0's binary_logloss: 0.314165\n",
      "[125]\tvalid_0's binary_logloss: 0.315157\n",
      "[126]\tvalid_0's binary_logloss: 0.315586\n",
      "[127]\tvalid_0's binary_logloss: 0.317212\n",
      "[128]\tvalid_0's binary_logloss: 0.317841\n",
      "[129]\tvalid_0's binary_logloss: 0.319013\n",
      "[130]\tvalid_0's binary_logloss: 0.319864\n",
      "[131]\tvalid_0's binary_logloss: 0.321202\n",
      "[132]\tvalid_0's binary_logloss: 0.32139\n",
      "[133]\tvalid_0's binary_logloss: 0.322776\n",
      "[134]\tvalid_0's binary_logloss: 0.323748\n",
      "[135]\tvalid_0's binary_logloss: 0.32327\n",
      "[136]\tvalid_0's binary_logloss: 0.324745\n",
      "[137]\tvalid_0's binary_logloss: 0.324732\n",
      "[138]\tvalid_0's binary_logloss: 0.326681\n",
      "[139]\tvalid_0's binary_logloss: 0.328459\n",
      "[140]\tvalid_0's binary_logloss: 0.329569\n",
      "[141]\tvalid_0's binary_logloss: 0.330331\n",
      "[142]\tvalid_0's binary_logloss: 0.330639\n",
      "[143]\tvalid_0's binary_logloss: 0.332737\n",
      "[144]\tvalid_0's binary_logloss: 0.333735\n",
      "[145]\tvalid_0's binary_logloss: 0.33552\n",
      "[146]\tvalid_0's binary_logloss: 0.337102\n",
      "[147]\tvalid_0's binary_logloss: 0.337981\n",
      "[148]\tvalid_0's binary_logloss: 0.337619\n",
      "[149]\tvalid_0's binary_logloss: 0.338584\n",
      "[150]\tvalid_0's binary_logloss: 0.339682\n",
      "[151]\tvalid_0's binary_logloss: 0.340175\n",
      "[152]\tvalid_0's binary_logloss: 0.342349\n",
      "[153]\tvalid_0's binary_logloss: 0.343471\n",
      "[154]\tvalid_0's binary_logloss: 0.344402\n",
      "[155]\tvalid_0's binary_logloss: 0.346625\n",
      "[156]\tvalid_0's binary_logloss: 0.347953\n",
      "[157]\tvalid_0's binary_logloss: 0.349127\n",
      "[158]\tvalid_0's binary_logloss: 0.350045\n",
      "[159]\tvalid_0's binary_logloss: 0.351787\n",
      "[160]\tvalid_0's binary_logloss: 0.35314\n",
      "[161]\tvalid_0's binary_logloss: 0.354176\n",
      "[162]\tvalid_0's binary_logloss: 0.355748\n",
      "[163]\tvalid_0's binary_logloss: 0.357275\n",
      "[164]\tvalid_0's binary_logloss: 0.358765\n",
      "[165]\tvalid_0's binary_logloss: 0.360803\n",
      "[166]\tvalid_0's binary_logloss: 0.361928\n",
      "[167]\tvalid_0's binary_logloss: 0.364352\n",
      "[168]\tvalid_0's binary_logloss: 0.365028\n",
      "[169]\tvalid_0's binary_logloss: 0.366569\n",
      "[170]\tvalid_0's binary_logloss: 0.366443\n",
      "[171]\tvalid_0's binary_logloss: 0.36648\n",
      "[172]\tvalid_0's binary_logloss: 0.367157\n",
      "[173]\tvalid_0's binary_logloss: 0.368807\n",
      "[174]\tvalid_0's binary_logloss: 0.37052\n",
      "[175]\tvalid_0's binary_logloss: 0.371658\n",
      "[176]\tvalid_0's binary_logloss: 0.371821\n",
      "[177]\tvalid_0's binary_logloss: 0.374335\n",
      "[178]\tvalid_0's binary_logloss: 0.376239\n",
      "[179]\tvalid_0's binary_logloss: 0.37665\n",
      "[180]\tvalid_0's binary_logloss: 0.3769\n",
      "[181]\tvalid_0's binary_logloss: 0.377061\n",
      "[182]\tvalid_0's binary_logloss: 0.378563\n",
      "[183]\tvalid_0's binary_logloss: 0.379002\n",
      "[184]\tvalid_0's binary_logloss: 0.379512\n",
      "[185]\tvalid_0's binary_logloss: 0.381727\n",
      "[186]\tvalid_0's binary_logloss: 0.382914\n",
      "[187]\tvalid_0's binary_logloss: 0.384131\n",
      "[188]\tvalid_0's binary_logloss: 0.387208\n",
      "[189]\tvalid_0's binary_logloss: 0.388376\n",
      "[190]\tvalid_0's binary_logloss: 0.389547\n",
      "[191]\tvalid_0's binary_logloss: 0.390172\n",
      "[192]\tvalid_0's binary_logloss: 0.391797\n",
      "[193]\tvalid_0's binary_logloss: 0.393282\n",
      "[194]\tvalid_0's binary_logloss: 0.394064\n",
      "[195]\tvalid_0's binary_logloss: 0.394255\n",
      "[196]\tvalid_0's binary_logloss: 0.394124\n",
      "[197]\tvalid_0's binary_logloss: 0.396004\n",
      "[198]\tvalid_0's binary_logloss: 0.398291\n",
      "[199]\tvalid_0's binary_logloss: 0.399875\n",
      "[200]\tvalid_0's binary_logloss: 0.401228\n",
      "[201]\tvalid_0's binary_logloss: 0.40183\n",
      "[202]\tvalid_0's binary_logloss: 0.402541\n",
      "[203]\tvalid_0's binary_logloss: 0.403635\n",
      "[204]\tvalid_0's binary_logloss: 0.40358\n",
      "[205]\tvalid_0's binary_logloss: 0.404196\n",
      "[206]\tvalid_0's binary_logloss: 0.406264\n",
      "[207]\tvalid_0's binary_logloss: 0.40748\n",
      "[208]\tvalid_0's binary_logloss: 0.408147\n",
      "[209]\tvalid_0's binary_logloss: 0.410255\n",
      "[210]\tvalid_0's binary_logloss: 0.412155\n",
      "[211]\tvalid_0's binary_logloss: 0.412569\n",
      "[212]\tvalid_0's binary_logloss: 0.413244\n",
      "[213]\tvalid_0's binary_logloss: 0.414225\n",
      "[214]\tvalid_0's binary_logloss: 0.414771\n",
      "[215]\tvalid_0's binary_logloss: 0.41621\n",
      "[216]\tvalid_0's binary_logloss: 0.417202\n",
      "[217]\tvalid_0's binary_logloss: 0.418366\n",
      "[218]\tvalid_0's binary_logloss: 0.418546\n",
      "[219]\tvalid_0's binary_logloss: 0.419096\n",
      "[220]\tvalid_0's binary_logloss: 0.42207\n",
      "[221]\tvalid_0's binary_logloss: 0.424008\n",
      "[222]\tvalid_0's binary_logloss: 0.424899\n",
      "[223]\tvalid_0's binary_logloss: 0.425196\n",
      "[224]\tvalid_0's binary_logloss: 0.42765\n",
      "[225]\tvalid_0's binary_logloss: 0.430378\n",
      "[226]\tvalid_0's binary_logloss: 0.43259\n",
      "[227]\tvalid_0's binary_logloss: 0.434274\n",
      "[228]\tvalid_0's binary_logloss: 0.436001\n",
      "[229]\tvalid_0's binary_logloss: 0.437217\n",
      "[230]\tvalid_0's binary_logloss: 0.439249\n",
      "[231]\tvalid_0's binary_logloss: 0.439986\n",
      "[232]\tvalid_0's binary_logloss: 0.441478\n",
      "[233]\tvalid_0's binary_logloss: 0.442339\n",
      "[234]\tvalid_0's binary_logloss: 0.443367\n",
      "[235]\tvalid_0's binary_logloss: 0.444185\n",
      "[236]\tvalid_0's binary_logloss: 0.446005\n",
      "[237]\tvalid_0's binary_logloss: 0.447037\n",
      "[238]\tvalid_0's binary_logloss: 0.447652\n",
      "[239]\tvalid_0's binary_logloss: 0.44804\n",
      "[240]\tvalid_0's binary_logloss: 0.450095\n",
      "[241]\tvalid_0's binary_logloss: 0.451491\n",
      "[242]\tvalid_0's binary_logloss: 0.453067\n",
      "[243]\tvalid_0's binary_logloss: 0.454223\n",
      "[244]\tvalid_0's binary_logloss: 0.455942\n",
      "[245]\tvalid_0's binary_logloss: 0.457342\n",
      "[246]\tvalid_0's binary_logloss: 0.459146\n",
      "[247]\tvalid_0's binary_logloss: 0.460036\n",
      "[248]\tvalid_0's binary_logloss: 0.462192\n",
      "[249]\tvalid_0's binary_logloss: 0.463151\n",
      "[250]\tvalid_0's binary_logloss: 0.465218\n",
      "[251]\tvalid_0's binary_logloss: 0.466708\n",
      "[252]\tvalid_0's binary_logloss: 0.467443\n",
      "[253]\tvalid_0's binary_logloss: 0.469669\n",
      "[254]\tvalid_0's binary_logloss: 0.471432\n",
      "[255]\tvalid_0's binary_logloss: 0.472225\n",
      "[256]\tvalid_0's binary_logloss: 0.472677\n",
      "[257]\tvalid_0's binary_logloss: 0.474475\n",
      "[258]\tvalid_0's binary_logloss: 0.475008\n",
      "[259]\tvalid_0's binary_logloss: 0.476024\n",
      "[260]\tvalid_0's binary_logloss: 0.478013\n",
      "[261]\tvalid_0's binary_logloss: 0.4797\n",
      "[262]\tvalid_0's binary_logloss: 0.48092\n",
      "[263]\tvalid_0's binary_logloss: 0.481647\n",
      "[264]\tvalid_0's binary_logloss: 0.482885\n",
      "[265]\tvalid_0's binary_logloss: 0.483654\n",
      "[266]\tvalid_0's binary_logloss: 0.484077\n",
      "[267]\tvalid_0's binary_logloss: 0.485454\n",
      "[268]\tvalid_0's binary_logloss: 0.486334\n",
      "[269]\tvalid_0's binary_logloss: 0.488018\n",
      "[270]\tvalid_0's binary_logloss: 0.487909\n",
      "[271]\tvalid_0's binary_logloss: 0.488638\n",
      "[272]\tvalid_0's binary_logloss: 0.489475\n",
      "[273]\tvalid_0's binary_logloss: 0.490018\n",
      "[274]\tvalid_0's binary_logloss: 0.491027\n",
      "[275]\tvalid_0's binary_logloss: 0.492647\n",
      "[276]\tvalid_0's binary_logloss: 0.494941\n",
      "[277]\tvalid_0's binary_logloss: 0.495458\n",
      "[278]\tvalid_0's binary_logloss: 0.497401\n",
      "[279]\tvalid_0's binary_logloss: 0.499041\n",
      "[280]\tvalid_0's binary_logloss: 0.500703\n",
      "[281]\tvalid_0's binary_logloss: 0.503619\n",
      "[282]\tvalid_0's binary_logloss: 0.50474\n",
      "[283]\tvalid_0's binary_logloss: 0.506614\n",
      "[284]\tvalid_0's binary_logloss: 0.507509\n",
      "[285]\tvalid_0's binary_logloss: 0.50648\n",
      "[286]\tvalid_0's binary_logloss: 0.50831\n",
      "[287]\tvalid_0's binary_logloss: 0.509712\n",
      "[288]\tvalid_0's binary_logloss: 0.510637\n",
      "[289]\tvalid_0's binary_logloss: 0.512079\n",
      "[290]\tvalid_0's binary_logloss: 0.514424\n",
      "[291]\tvalid_0's binary_logloss: 0.516105\n",
      "[292]\tvalid_0's binary_logloss: 0.518416\n",
      "[293]\tvalid_0's binary_logloss: 0.519068\n",
      "[294]\tvalid_0's binary_logloss: 0.520845\n",
      "[295]\tvalid_0's binary_logloss: 0.523428\n",
      "[296]\tvalid_0's binary_logloss: 0.525182\n",
      "[297]\tvalid_0's binary_logloss: 0.526087\n",
      "[298]\tvalid_0's binary_logloss: 0.527105\n",
      "[299]\tvalid_0's binary_logloss: 0.529075\n",
      "[300]\tvalid_0's binary_logloss: 0.530392\n"
     ]
    }
   ],
   "source": [
    "lgbm_classifier = lgb.train(parameters,\n",
    "                       train_data,\n",
    "                       valid_sets=test_data,\n",
    "                       num_boost_round=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W78pbOqTuvGh"
   },
   "outputs": [],
   "source": [
    "y_hat = lgbm_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "anci99rWv2ht",
    "outputId": "cce80ca7-fd7a-4a4c-b670-150c90ce8faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbDNWdDnvB8A",
    "outputId": "139fb3ca-e7a0-4352-858c-56b6b2fccd7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_hat.round()==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ye3J0Xg0Bkko",
    "outputId": "ec7a5ce9-7e65-45c5-f720-69cf5ebd5c64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.933     0.903     0.918       248\n",
      "         1.0      0.908     0.937     0.922       252\n",
      "\n",
      "    accuracy                          0.920       500\n",
      "   macro avg      0.921     0.920     0.920       500\n",
      "weighted avg      0.920     0.920     0.920       500\n",
      "\n",
      "[[224  24]\n",
      " [ 16 236]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_hat.round(), digits=3))\n",
    "print(confusion_matrix(y_test, y_hat.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbkWwRx5BqF4"
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogcZnorjBrJ7"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC(C=9.0622635,\n",
    "          kernel='rbf',\n",
    "          gamma='scale',\n",
    "          coef0=0.0,\n",
    "          tol=0.001,\n",
    "          probability=True,\n",
    "          max_iter=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSkHyPdfBt5v",
    "outputId": "dfed202d-0104-4400-fecb-817ed750ea40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=9.0622635, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZnyduhaBw_Y"
   },
   "outputs": [],
   "source": [
    "svm_pred = svm_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-o_M5fQzCBce",
    "outputId": "daded25a-f6ee-4909-d882-dc5a772c36f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(svm_pred.round()==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eP4a7L3qCEg9",
    "outputId": "b90963b5-1abc-46aa-fc1c-aa52d1bc4575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.944     0.891     0.917       248\n",
      "         1.0      0.898     0.948     0.923       252\n",
      "\n",
      "    accuracy                          0.920       500\n",
      "   macro avg      0.921     0.920     0.920       500\n",
      "weighted avg      0.921     0.920     0.920       500\n",
      "\n",
      "[[221  27]\n",
      " [ 13 239]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, svm_pred.round(), digits=3))\n",
    "print(confusion_matrix(y_test, svm_pred.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik1vjAYuURo-"
   },
   "source": [
    "### Perceptron NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78lnHAv5DDOd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, x , y):\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.x[idx].astype('float32')).to(device)\n",
    "        y = torch.tensor(self.y[idx].astype('float32')).unsqueeze(0).to(device)\n",
    "        return data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93-kIeh_zxW9"
   },
   "outputs": [],
   "source": [
    "train_ds = Text(x, y)\n",
    "train_loader = DataLoader(dataset=train_ds, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LHY5faTTNB5"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BasicModel, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim  = output_dim\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wY2qWHdORCRq"
   },
   "outputs": [],
   "source": [
    "basic_classifier = BasicModel(input_dim=512*1, hidden_dim=50, output_dim=1).to(device)\n",
    "c = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(basic_classifier.parameters(), lr=0.001)\n",
    "\n",
    "train_loss_history = []\n",
    "val_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lYCLAPqRMWo",
    "outputId": "0b1fa66d-c3c7-4f1a-f8fd-3265aa32acdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [2/32], Loss for last 2 batches: 0.6663\n",
      "Epoch [1/3], Step [4/32], Loss for last 2 batches: 0.6241\n",
      "Epoch [1/3], Step [6/32], Loss for last 2 batches: 0.6275\n",
      "Epoch [1/3], Step [8/32], Loss for last 2 batches: 0.6386\n",
      "Epoch [1/3], Step [10/32], Loss for last 2 batches: 0.6317\n",
      "Epoch [1/3], Step [12/32], Loss for last 2 batches: 0.6243\n",
      "Epoch [1/3], Step [14/32], Loss for last 2 batches: 0.6200\n",
      "Storing with loss for last 15 batches = 0.6331382433573405\n",
      "Epoch [1/3], Step [16/32], Loss for last 2 batches: 0.6284\n",
      "Epoch [1/3], Step [18/32], Loss for last 2 batches: 0.6207\n",
      "Epoch [1/3], Step [20/32], Loss for last 2 batches: 0.6145\n",
      "Epoch [1/3], Step [22/32], Loss for last 2 batches: 0.6066\n",
      "Epoch [1/3], Step [24/32], Loss for last 2 batches: 0.6115\n",
      "Epoch [1/3], Step [26/32], Loss for last 2 batches: 0.6225\n",
      "Epoch [1/3], Step [28/32], Loss for last 2 batches: 0.5810\n",
      "Epoch [1/3], Step [30/32], Loss for last 2 batches: 0.5960\n",
      "Storing with loss for last 15 batches = 0.6086814324061076\n",
      "Epoch [1/3], Step [32/32], Loss for last 2 batches: 0.5956\n",
      "Epoch [1/3] finished with loss = 0.6193\n",
      "Epoch [2/3], Step [2/32], Loss for last 2 batches: 0.5861\n",
      "Epoch [2/3], Step [4/32], Loss for last 2 batches: 0.6064\n",
      "Epoch [2/3], Step [6/32], Loss for last 2 batches: 0.5946\n",
      "Epoch [2/3], Step [8/32], Loss for last 2 batches: 0.5928\n",
      "Epoch [2/3], Step [10/32], Loss for last 2 batches: 0.5976\n",
      "Epoch [2/3], Step [12/32], Loss for last 2 batches: 0.5854\n",
      "Epoch [2/3], Step [14/32], Loss for last 2 batches: 0.5636\n",
      "Storing with loss for last 15 batches = 0.5882914900779724\n",
      "Epoch [2/3], Step [16/32], Loss for last 2 batches: 0.5743\n",
      "Epoch [2/3], Step [18/32], Loss for last 2 batches: 0.5655\n",
      "Epoch [2/3], Step [20/32], Loss for last 2 batches: 0.5989\n",
      "Epoch [2/3], Step [22/32], Loss for last 2 batches: 0.5449\n",
      "Epoch [2/3], Step [24/32], Loss for last 2 batches: 0.5792\n",
      "Epoch [2/3], Step [26/32], Loss for last 2 batches: 0.5756\n",
      "Epoch [2/3], Step [28/32], Loss for last 2 batches: 0.5650\n",
      "Epoch [2/3], Step [30/32], Loss for last 2 batches: 0.5735\n",
      "Storing with loss for last 15 batches = 0.5721512317657471\n",
      "Epoch [2/3], Step [32/32], Loss for last 2 batches: 0.5595\n",
      "Epoch [2/3] finished with loss = 0.5789\n",
      "Epoch [3/3], Step [2/32], Loss for last 2 batches: 0.5870\n",
      "Epoch [3/3], Step [4/32], Loss for last 2 batches: 0.5725\n",
      "Epoch [3/3], Step [6/32], Loss for last 2 batches: 0.5680\n",
      "Epoch [3/3], Step [8/32], Loss for last 2 batches: 0.5577\n",
      "Epoch [3/3], Step [10/32], Loss for last 2 batches: 0.5718\n",
      "Epoch [3/3], Step [12/32], Loss for last 2 batches: 0.5712\n",
      "Epoch [3/3], Step [14/32], Loss for last 2 batches: 0.5594\n",
      "Storing with loss for last 15 batches = 0.5688205560048422\n",
      "Epoch [3/3], Step [16/32], Loss for last 2 batches: 0.5661\n",
      "Epoch [3/3], Step [18/32], Loss for last 2 batches: 0.5396\n",
      "Epoch [3/3], Step [20/32], Loss for last 2 batches: 0.5597\n",
      "Epoch [3/3], Step [22/32], Loss for last 2 batches: 0.5714\n",
      "Epoch [3/3], Step [24/32], Loss for last 2 batches: 0.5674\n",
      "Epoch [3/3], Step [26/32], Loss for last 2 batches: 0.5935\n",
      "Epoch [3/3], Step [28/32], Loss for last 2 batches: 0.5570\n",
      "Epoch [3/3], Step [30/32], Loss for last 2 batches: 0.5656\n",
      "Storing with loss for last 15 batches = 0.5655714631080627\n",
      "Epoch [3/3], Step [32/32], Loss for last 2 batches: 0.5758\n",
      "Epoch [3/3] finished with loss = 0.5677\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = len(train_loader)\n",
    "num_epochs = 3\n",
    "initial_epoch = 1\n",
    "log_nth = 2\n",
    "storing_frequency = 15\n",
    "checkpoints_path = \"/content/drive/MyDrive/ExplainableAI/Model/Saliency/checkpoints\"\n",
    "\n",
    "for epoch in range(initial_epoch, initial_epoch+num_epochs):\n",
    "    basic_classifier.train()\n",
    "    epoch_losses = []\n",
    "    for i, (data, y_label) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      out = basic_classifier(data)\n",
    "      loss = c(out, y_label)\n",
    "      epoch_losses.append(loss.item())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if (i+1) % log_nth == 0:        \n",
    "          print ('Epoch [{}/{}], Step [{}/{}], Loss for last {} batches: {:.4f}' \n",
    "                  .format(epoch, num_epochs, i+1, iter_per_epoch, log_nth, np.mean(np.array(epoch_losses[-log_nth:]))))\n",
    "          #print_time()\n",
    "      \n",
    "      if (i+1) % storing_frequency == 0:        \n",
    "          print('Storing with loss for last {} batches = {}'.format(storing_frequency, np.mean(np.array(epoch_losses[-storing_frequency:]))))\n",
    "          #print_time()\n",
    "          #torch.save(basic_classifier.state_dict(), checkpoints_path+\"/final_model_epoch_{}_{}.checkpoint\".format(epoch, i+1))\n",
    "  \n",
    "    # Store after whole epoch\n",
    "    print ('Epoch [{}/{}] finished with loss = {:.4f}'.format(epoch, num_epochs, np.mean(np.array(epoch_losses))))\n",
    "    #torch.save(basic_classifier.state_dict(), checkpoints_path+\"/final_model_epoch_{}.checkpoint\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUN1zRwzDVmm"
   },
   "outputs": [],
   "source": [
    "nn_pred = basic_classifier(torch.tensor(x_test.astype('float32')).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvTcF71WDVmn"
   },
   "outputs": [],
   "source": [
    "nn_pred = nn_pred.flatten().detach().cpu().numpy().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iz0OohekDVmo",
    "outputId": "c8f116e4-8553-4f37-960b-d94893013789"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nn_pred==y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGgUvOOkDVmp",
    "outputId": "615a6c93-8a71-41a5-efac-48747d1a0df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.885     0.927     0.906       248\n",
      "         1.0      0.925     0.881     0.902       252\n",
      "\n",
      "    accuracy                          0.904       500\n",
      "   macro avg      0.905     0.904     0.904       500\n",
      "weighted avg      0.905     0.904     0.904       500\n",
      "\n",
      "[[230  18]\n",
      " [ 30 222]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, nn_pred, digits=3))\n",
    "print(confusion_matrix(y_test, nn_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Training_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
